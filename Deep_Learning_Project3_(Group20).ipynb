{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning - Project3 (Group20).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oX-fPfkZw-nN"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "data_loc = \"/content/drive/MyDrive/Deep_Learning\"\n",
        "sys.path.append(os.path.abspath(data_loc))"
      ],
      "metadata": {
        "id": "OzE2LxC4xBGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import time\n",
        "from platform import python_version\n",
        "import gc\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable"
      ],
      "metadata": {
        "id": "bbOeCTA1xBJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_excel(\"/content/drive/MyDrive/Deep_Learning/data/train_sample_30K.xlsx\")\n",
        "data_og = pd.read_csv(\"/content/drive/MyDrive/Deep_Learning/data/train.csv\")\n",
        "df_v2 = pd.merge(data[['id']], data_og, on='id')\n",
        "df = df_v2.sort_values(by=['id']).reset_index(drop=True)\n",
        "df['toxic_binary'] = np.where((df['toxic']==1)|(df['severe_toxic']==1)|(df['obscene']==1)|\n",
        "                              (df['threat']==1)|(df['insult']==1)|(df['identity_hate']==1), 1, 0)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "wN_AypssxBL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Feedforward(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size, dropout):\n",
        "        super(Feedforward, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.fc2 = torch.nn.Linear(self.hidden_size, 6)\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.dropout(x)\n",
        "        hidden = self.fc1(x)\n",
        "        relu = self.relu(hidden)\n",
        "        relu = self.dropout(relu)\n",
        "        output = self.fc2(relu)\n",
        "        output = self.sigmoid(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "class KimCNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, embed_num, embed_dim, class_num, kernel_num, kernel_sizes, dropout, static):\n",
        "        super(KimCNN, self).__init__()\n",
        "        V = embed_num\n",
        "        D = embed_dim\n",
        "        C = class_num\n",
        "        Co = kernel_num\n",
        "        Ks = kernel_sizes\n",
        "        self.static = static\n",
        "        self.embed = nn.Embedding(V, D)\n",
        "        self.convs1 = nn.ModuleList([nn.Conv2d(1, Co, (K, D)) for K in Ks])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc1 = nn.Linear(len(Ks) * Co, C)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        if self.static:\n",
        "            x = Variable(x)\n",
        "        x = x.unsqueeze(1)\n",
        "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]\n",
        "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n",
        "        x = torch.cat(x, 1)\n",
        "        x = self.dropout(x)\n",
        "        logit = self.fc1(x)\n",
        "        output = self.sigmoid(logit)\n",
        "        return output\n",
        "\n",
        "\n",
        "class GRUNet(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob):\n",
        "        super(GRUNet, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, bidirectional=True, dropout=drop_prob)\n",
        "        self.fc = nn.Linear(2*hidden_dim, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, x, h):\n",
        "        out, h = self.gru(x, h)\n",
        "        out = self.fc(self.relu(out[:,-1]))\n",
        "        out = self.sigmoid(out)\n",
        "        return out, h\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = weight.new(self.n_layers*2, batch_size, self.hidden_dim).zero_()\n",
        "        return hidden"
      ],
      "metadata": {
        "id": "5kQEgOH4xBPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Choose one model and comment the other lines---------------------------------\n",
        "model = Feedforward(\n",
        "    input_size=76800,\n",
        "    hidden_size=1000,\n",
        "    dropout=0.7\n",
        ")\n",
        "\n",
        "model = KimCNN(\n",
        "    embed_num = 400\n",
        "    embed_dim = 768\n",
        "    class_num = 6\n",
        "    kernel_num = 3\n",
        "    kernel_sizes = [2, 3, 4]\n",
        "    dropout = 0.7\n",
        "    static = True\n",
        ")\n",
        "\n",
        "model = GRUNet(\n",
        "    input_dim=100,\n",
        "    hidden_dim=768,\n",
        "    output_dim=6,\n",
        "    n_layers=1,\n",
        "    drop_prob=0.7\n",
        ")"
      ],
      "metadata": {
        "id": "uASdB9ijxBRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 5\n",
        "batch_size = 50\n",
        "lr = 0.0001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "loss_fn = nn.BCELoss()"
      ],
      "metadata": {
        "id": "8ySNsabRyQTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_batch_data(x, y, batch_size):\n",
        "    i, batch = 0, 0\n",
        "    for batch, i in enumerate(range(0, len(x) - batch_size, batch_size), 1):\n",
        "        x_batch = x[i : i + batch_size]\n",
        "        y_batch = y[i : i + batch_size]\n",
        "        yield x_batch, y_batch, batch\n",
        "    if i + batch_size < len(x):\n",
        "        yield x[i + batch_size :], y[i + batch_size :], batch + 1\n",
        "    if batch == 0:\n",
        "        yield x, y, 1"
      ],
      "metadata": {
        "id": "XKAwdJ-xxBbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODEL TRAINING\n",
        "- Choose one model and comment the other lines in the code (look for comments to understand what lines to comment)"
      ],
      "metadata": {
        "id": "U7Xhrl68z7Wh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Choose one model and comment the other lines in the code (look for comments to understand what lines to comment)\n",
        "train_losses, val_losses = [], []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    start_time = time.time()\n",
        "    train_loss = 0\n",
        "    h = model.init_hidden(batch_size)\n",
        "\n",
        "    for i in range(0,25):\n",
        "        if (i+1)%5==0:\n",
        "            print(\"Epoch %d - data subset %d\"%(epoch+1, i+1))\n",
        "        file_to_read = \"/content/drive/MyDrive/Deep_Learning/data_tensors/embed_\"+str(i)+\"_12.pt\"\n",
        "        x_train = torch.load(file_to_read)\n",
        "\n",
        "        #CHOOSE ONE MODEL AND COMMENT THE OTHER LINES---------------------------------\n",
        "        #Method 1 - FeedForward Neural Network\n",
        "        x_train = x_train.view(x_train.size(0), -1)\n",
        "        #Method 2 - Kim-CNN\n",
        "        x_train = x_train\n",
        "        #Method 3 - Bi-GRU\n",
        "        x_train = x_train.transpose(-2, -1)\n",
        "\n",
        "        y_train = torch.tensor(df[i*1000:((i+1)*1000)][['toxic','severe_toxic','obscene','threat','insult','identity_hate']].values.astype(np.float32))\n",
        "\n",
        "        for x_batch, y_batch, batch in generate_batch_data(x_train, y_train, batch_size):\n",
        "            \n",
        "            #CHOOSE ONE MODEL AND COMMENT THE OTHER LINES---------------------------------\n",
        "            #Method 1 - FeedForward Neural Network\n",
        "            y_pred = model(x_batch)\n",
        "            #Method 2 - Kim-CNN\n",
        "            y_pred = model(x_batch)\n",
        "            #Method 3 - Bi-GRU\n",
        "            h = h.data\n",
        "            y_pred, h = model(x_batch, h)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            loss = loss_fn(y_pred, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "    train_loss /= (batch*(i+1))\n",
        "    train_losses.append(train_loss)\n",
        "    elapsed = time.time() - start_time\n",
        "\n",
        "    print(\n",
        "        \"Epoch %d Train loss: %.2f. Elapsed time: %.2fs.\"\n",
        "        % (epoch + 1, train_losses[-1], elapsed)\n",
        "    )"
      ],
      "metadata": {
        "id": "di-KU5Z0xBdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODEL VALIDATION\n",
        "- Choose one model and comment the other lines in the code (look for comments to understand what lines to comment)"
      ],
      "metadata": {
        "id": "EW0XZX9T0DGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval() # disable dropout for deterministic output\n",
        "\n",
        "with torch.no_grad(): # deactivate autograd engine to reduce memory usage and speed up computations\n",
        "    y_preds = []\n",
        "    batch = 0\n",
        "    for i in range(0,30):\n",
        "        if (i+1)%5==0:\n",
        "            print(\"Epoch %d - data subset %d\"%(epoch+1, i+1))\n",
        "\n",
        "        # test = [0]*len(range(9,13))\n",
        "        # for j in range(9,13):\n",
        "        #     file_to_read = \"/content/drive/MyDrive/Deep_Learning/data_tensors/embed_\"+str(i)+\"_\"+str(j)+\".pt\"\n",
        "        #     test[j-9] = torch.load(file_to_read)\n",
        "            \n",
        "\n",
        "        # x_test = torch.cat(test, dim=1)\n",
        "        file_to_read = \"/content/drive/MyDrive/Deep_Learning/data_tensors/embed_\"+str(i)+\"_12.pt\"\n",
        "        x_test = torch.load(file_to_read)\n",
        "\n",
        "        #CHOOSE ONE MODEL AND COMMENT THE OTHER LINES---------------------------------\n",
        "        #Method 1 - FeedForward Neural Network\n",
        "        x_test = x_test.view(x_test.size(0), -1)\n",
        "        #Method 2 - Kim-CNN\n",
        "        x_test = x_test\n",
        "        #Method 3 - Bi-GRU\n",
        "        x_test = x_test.transpose(-2, -1)\n",
        "\n",
        "        y_test = torch.tensor(df[i*1000:((i+1)*1000)][['toxic','severe_toxic','obscene','threat','insult','identity_hate']].values.astype(np.float32))\n",
        "\n",
        "        for x_batch, y_batch, batch in generate_batch_data(x_test, y_test, batch_size):\n",
        "            \n",
        "            #CHOOSE ONE MODEL AND COMMENT THE OTHER LINES---------------------------------\n",
        "            #Method 1 - FeedForward Neural Network\n",
        "            y_pred = model(x_batch)\n",
        "            #Method 2 - Kim-CNN\n",
        "            y_pred = model(x_batch)\n",
        "            #Method 3 - Bi-GRU\n",
        "            h = model.init_hidden(x_batch.shape[0])\n",
        "            y_pred, h = model(x_batch, h)\n",
        "\n",
        "            y_preds.extend(y_pred.cpu().numpy().tolist())\n",
        "    \n",
        "    y_preds_np = np.array(y_preds)"
      ],
      "metadata": {
        "id": "5G2u9QX8xBiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the target values\n",
        "y_test_np = df[['toxic','severe_toxic','obscene','threat','insult','identity_hate']].values"
      ],
      "metadata": {
        "id": "uEjxGxSPxBki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compute the training AUC\n",
        "auc_scores = roc_auc_score(y_test_np[0:25000], y_preds_np[0:25000], average=None)\n",
        "df_accuracy = pd.DataFrame({\"label\": ['toxic','severe_toxic','obscene','threat','insult','identity_hate'], \"auc\": auc_scores})\n",
        "df_accuracy.sort_values('auc')[::-1]"
      ],
      "metadata": {
        "id": "FCUADi2ZxBmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compute the test AUC\n",
        "auc_scores = roc_auc_score(y_test_np[25000:], y_preds_np[25000:], average=None)\n",
        "df_accuracy = pd.DataFrame({\"label\": ['toxic','severe_toxic','obscene','threat','insult','identity_hate'], \"auc\": auc_scores})\n",
        "df_accuracy.sort_values('auc')[::-1]"
      ],
      "metadata": {
        "id": "GbOHFhN0xBoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rrja2HBLxBqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MOjcmUVXxCTx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}